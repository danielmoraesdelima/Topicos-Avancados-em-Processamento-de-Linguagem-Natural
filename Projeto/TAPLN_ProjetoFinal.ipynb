{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC8QtjIAGtEE"
      },
      "source": [
        "#### TRABALHO FINAL DA DISCIPLINA − TÓPICOS AVANÇADOS EM PLN - 2021.2\n",
        "\n",
        "Aluno: Daniel Moraes\n",
        "\n",
        "Objetivos: Desenvolver o módulo de NLU (Natural Language Understanding) de um sistema para chatbot fim-a-fim que recebe dois textos em linguagem natural e define se o 2o texto é resposta para o 1o.texto.\n",
        "\n",
        "Será fornecido um dataset com 180000 pares de sentenças que representam diálogos positivos, em inglês. Os diálogos negativos deverão ser “montados” pela equipe.\n",
        "\n",
        "As equipes serão avaliadas pelos entregáveis e pelos resultados a partir de um conjunto de teste (dados não vistos – Held-Out Data) com 18000 pares de sentenças.\n",
        "\n",
        "#### Produtos Entregáveis:\n",
        "\n",
        "18/10/2021\n",
        "\n",
        "- Pré-processamento, Lematização e POS Tagger, NER, etc.\n",
        "- Estatística Descritiva do vocabulário constante no dataset, considerando os modelos de linguagem unigrama, bigrama, trigrama; analise da lei de potencia ou Lei de Zipf.\n",
        "- Vetorização das Sentenças (features)\n",
        "\n",
        "06/12/2021\n",
        "\n",
        "- Model Fitted e Resultados do processo de treinamento\n",
        "- Execução com Held-Out-Data (Dados não vistos)\n",
        "- Artigo resumo com 4 paginas\n",
        "- Apresentação Oral e Entrega do artigo: 06/12/2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPcAP3uCGtEL"
      },
      "source": [
        "#### IMPORTACAO E INSTALAÇÃO DE BIBLIOTECAS, GPUS ETC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyGv46UNyNYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cfd953-0016-4b62-ab42-f3d093916b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan  4 16:14:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB1DfEpMgxpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b99e4e3-011f-4825-b6ee-2f803957c255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/jupyter-serverextension\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py\", line 267, in launch_instance\n",
            "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/notebook/serverextensions.py\", line 293, in start\n",
            "    super(ServerExtensionApp, self).start()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py\", line 256, in start\n",
            "    self.subapp.start()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/notebook/serverextensions.py\", line 210, in start\n",
            "    self.toggle_server_extension_python(arg)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/notebook/serverextensions.py\", line 199, in toggle_server_extension_python\n",
            "    m, server_exts = _get_server_extension_metadata(package)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/notebook/serverextensions.py\", line 327, in _get_server_extension_metadata\n",
            "    m = import_item(module)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/utils/importstring.py\", line 42, in import_item\n",
            "    return __import__(parts[0])\n",
            "ImportError: No module named jupyter_http_over_ws\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade jupyter_http_over_ws>=0.0.7 && \\\n",
        "  jupyter serverextension enable --py jupyter_http_over_ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ban5ByDwyg-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e83cc1ff-58c8-4e82-b4d1-b1c5adad6764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0ja-hVwUSv_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import string\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates \n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xgs8l4B9IaFO"
      },
      "outputs": [],
      "source": [
        "#Pacotes NLTK\n",
        "import nltk\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import string\n",
        "nltk.download('maxent_ne_chunker', quiet=True)\n",
        "nltk.download('words', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('rslp', quiet=True)\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords as sw\n",
        "from contextlib import redirect_stdout\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import RSLPStemmer\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import RSLPStemmer\n",
        "from nltk.text import TextCollection\n",
        "from collections import defaultdict\n",
        "import os\n",
        "from pprint import pprint\n",
        "import joblib\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "#Download das stopwords\n",
        "with redirect_stdout(open(os.devnull, \"w\")):\n",
        "    nltk.download(\"stopwords\", quiet=True) \n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download(\"all\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDiWE97GUqCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3722163-9dd4-4305-8115-423db6e6113a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |████████                        | 10 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 129 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.3)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30534 sha256=537c62faaee5859efcf5719eebd39aeb6ad1b0831b495c40053bea9db9b3b9c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19473 sha256=2a1e87b9cf855ef4a1019d2b37df9b60545bc8dfe1dec50f7ec64f0996a3adb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7912 sha256=3f259cf8f9f835710122a25429efa45485190c19def64d60c54e865586211b78\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 427 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n",
            "Collecting tensorflow==2.2.0-rc3\n",
            "  Downloading tensorflow-2.2.0rc3-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 9.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (0.37.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (1.4.1)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (1.42.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0rc0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 56.8 MB/s \n",
            "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (0.12.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (1.13.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0-rc3) (1.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (3.3.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (3.1.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-2.2.0rc3 tensorflow-estimator-2.2.0\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece\n",
        "!python3 -m spacy download en\n",
        "!pip install transformers\n",
        "!pip install tensorflow==2.2.0-rc3\n",
        "!pip install torchvision "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U tensorflow-text"
      ],
      "metadata": {
        "id": "LEZ8yxbzTlQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9002bc70-661f-4692-b6e7-f19f2f942589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 489.6 MB 22 kB/s \n",
            "\u001b[K     |████████████████████████████████| 463 kB 51.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 39.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWr4zpMnUuTM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "831255d0-259f-4577-88de-f37a1f4fa87e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "import bert\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tf-nightly"
      ],
      "metadata": {
        "id": "9mfiRKk-V03Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09d9d82b-bb19-4b2f-f60f-254477886177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-nightly\n",
            "  Downloading tf_nightly-2.9.0.dev20220104-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (487.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 487.9 MB 12 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.10.0.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.0)\n",
            "Collecting tb-nightly~=2.8.0.a\n",
            "  Downloading tb_nightly-2.8.0a20220104-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (12.0.0)\n",
            "Collecting tf-estimator-nightly~=2.9.0.dev\n",
            "  Downloading tf_estimator_nightly-2.9.0.dev2022010409-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 33.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.42.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.12.0)\n",
            "Collecting keras-nightly~=2.9.0.dev\n",
            "  Downloading keras_nightly-2.9.0.dev2022010408-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.8.0.a->tf-nightly) (1.8.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.8.0.a->tf-nightly) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.8.0.a->tf-nightly) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.8.0.a->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.8.0.a->tf-nightly) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.8.0.a->tf-nightly) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.8.0.a->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.8.0.a->tf-nightly) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.8.0.a->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.8.0.a->tf-nightly) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.8.0.a->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.8.0.a->tf-nightly) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly~=2.8.0.a->tf-nightly) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.8.0.a->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.8.0.a->tf-nightly) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.8.0.a->tf-nightly) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.8.0.a->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.8.0.a->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.8.0.a->tf-nightly) (3.1.1)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow-io-gcs-filesystem, tb-nightly, keras-nightly, tf-nightly\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.22.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.22.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.22.0\n",
            "Successfully installed keras-nightly-2.9.0.dev2022010408 tb-nightly-2.8.0a20220104 tensorflow-io-gcs-filesystem-0.23.1 tf-estimator-nightly-2.9.0.dev2022010409 tf-nightly-2.9.0.dev20220104\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Input"
      ],
      "metadata": {
        "id": "Lg2UEiJ5V6er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18u8ektgEIMh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f078ffad-961b-43df-a037-da1b59e0c215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.4.1\n",
            "  Downloading tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3 MB 12 kB/s \n",
            "\u001b[?25hCollecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.37.0)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 50.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.7.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.6.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.3.0)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.19.5)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.3.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.1)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68719 sha256=90c875117b879962e23c12697f5ce30384cd9b7c37504cef62ca765ff3961760\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, grpcio, wrapt, tensorflow-estimator, flatbuffers, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.42.0\n",
            "    Uninstalling grpcio-1.42.0:\n",
            "      Successfully uninstalled grpcio-1.42.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.7.3 requires tensorflow<2.8,>=2.7.0, but you have tensorflow 2.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed flatbuffers-1.12 grpcio-1.32.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flatbuffers",
                  "tensorflow",
                  "tensorflow_estimator",
                  "typing_extensions",
                  "wrapt"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "!pip install tensorflow==2.4.1\n",
        "#!pip install keras==2.4.3\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebVbWPuz8iLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceae50e4-4df7-4b2e-f85c-24ff7daeeab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44U-7_y3GtEV"
      },
      "source": [
        "#### Produtos Entregáveis:\n",
        "\n",
        "18/10/2021\n",
        "\n",
        "- Pré-processamento, Lematização e POS Tagger, NER, etc.\n",
        "- Estatística Descritiva do vocabulário constante no dataset, considerando os modelos de linguagem unigrama, bigrama, trigrama; analise da lei de potencia ou Lei de Zipf.\n",
        "- Vetorização das Sentenças (features)\n",
        "\n",
        "#### Tratamento inicial do texto\n",
        "\n",
        "Converte o texto de cada sentença, separadamente, em minúsculo e remove espaços e tabulações extras. O resultado é guardado no DataFrame referente a cada sentenca em uma nova coluna.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC2V2ql5Jm_T"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccim8p3sIgoD"
      },
      "outputs": [],
      "source": [
        "path = r\"/content/drive/MyDrive/Colab Notebooks/TopicosPLN/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPippHvPUuT8"
      },
      "outputs": [],
      "source": [
        "#Importando o Dataset\n",
        "#df = pd.read_csv(path+'ChatBotDataset.xlsx', delimiter=';', encoding= 'mac_roman')\n",
        "df = pd.read_excel(path+'ChatBotDataset.xlsx')\n",
        "\n",
        "#conversão da coluna 'pair_ID' de inteiro para string\n",
        "df['pair_ID'] = df['pair_ID'].astype('str')\n",
        "\n",
        "#Visualização do Cabeçalho dos dados\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oul1956NSu9_"
      },
      "outputs": [],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FllNQws6H3ZK"
      },
      "outputs": [],
      "source": [
        "#Função para tratar o texto\n",
        "def tratamento_texto(sentence):\n",
        "\n",
        "  #remove as quebras de linha\n",
        "  sentence = re.sub(r'\\n', '', sentence)\n",
        "  #substitui tabulações por um espaço em branco\n",
        "  sentence = re.sub(r'\\t', ' ', sentence)\n",
        "  #substitui um ou mais espaços em branco por um espaço\n",
        "  sentence= re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
        "  #remove aspas e apóstofres\n",
        "  sentence = re.sub('[\"‘’“”…]', '', sentence)  \n",
        "\n",
        "  #Principais remoções\n",
        "  sentence = sentence.replace('\\\\r', '')\n",
        "  sentence = sentence.replace('\\\\n', '')\n",
        "  \n",
        "  #remove o b'\n",
        "  sentence = sentence.replace(\"b'\", \"\")\n",
        "  sentence = sentence.replace(\"''\", \"\")\n",
        "  #remove o \\r\\n'\n",
        "  sentence= re.sub(\"[\\r\\n']\", ' ', sentence)\n",
        "  sentence= re.sub(\"[\\r\\n]\", ' ', sentence) \n",
        "\n",
        "  sentence= re.sub(\"[!,]\", ' ', sentence)\n",
        "  sentence= re.sub(\"[!.]\", ' ', sentence) \n",
        "  sentence= re.sub(\"[?.]\", ' ', sentence)\n",
        "  sentence= re.sub(\"[?,]\", ' ', sentence) \n",
        "\n",
        "  return sentence\n",
        "\n",
        "#cria uma nova coluna no dataframe 'sentence' com cada sentence tokenizado\n",
        "df['message_tratado'] = df['message'].apply(tratamento_texto)\n",
        "df['response_tratado'] = df['response'].apply(tratamento_texto)\n",
        "#Exibe o resultado\n",
        "df[df.columns[::-1]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj2r7AChIAvd"
      },
      "outputs": [],
      "source": [
        "#Funcao para separar as sentenças\n",
        "def separa_sentencas(texto):\n",
        "  lista_sentencas = sent_tokenize(texto)\n",
        "  nova_lista = []\n",
        "  for sent in lista_sentencas:\n",
        "    nova_lista.append(sent.strip())\n",
        "  return nova_lista\n",
        "\n",
        "#cria uma nova coluna no dataframe 'sentence' com cada sentence tokenizado\n",
        "df['message_em_sentencas'] = df['message_tratado'].apply(separa_sentencas)\n",
        "df['response_em_sentencas'] = df['response_tratado'].apply(separa_sentencas)\n",
        "#Exibe o resultado\n",
        "df[df.columns[::-1]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpQeA0t5IA9Q"
      },
      "outputs": [],
      "source": [
        "#Funcao para tokenizar - separar as sentenças\n",
        "def tokeniza_sentenca(lista_sentencas):\n",
        "  tokens = ''\n",
        "  for i in range (len(lista_sentencas)):\n",
        "    sentencas_unidas = \" \".join(w for w in lista_sentencas)\n",
        "    sentence = sent_tokenize(sentencas_unidas)\n",
        "    tokens = word_tokenize(sentence[i])\n",
        "  return tokens\n",
        "\n",
        "#cria uma nova coluna no dataframe 'sentence' com cada sentence tokenizado\n",
        "df['message_tokenizado'] = df['message_em_sentencas'].apply(tokeniza_sentenca)\n",
        "df['response_tokenizado'] = df['response_em_sentencas'].apply(tokeniza_sentenca)\n",
        "#Exibe o resultado\n",
        "df[df.columns[::-1]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_89o7xmIBBH"
      },
      "outputs": [],
      "source": [
        "#Remover palavras que não são importantes para a contabilizaçao\n",
        "def remove_stop_words(lista_tokens):\n",
        "  stopwords = sw.words('english')\n",
        "  stop_words = set(stopwords + list(string.punctuation))\n",
        "  tokens = [w for w in lista_tokens if not w in stop_words]\n",
        "  return tokens\n",
        "\n",
        "#cria uma nova coluna no dataframe sem stopwords\n",
        "df['message_sem_stopwords'] = df['message_tokenizado'].apply(remove_stop_words)\n",
        "df['response_sem_stopwords'] = df['response_tokenizado'].apply(remove_stop_words)\n",
        "#Exibe o resultado\n",
        "df[df.columns[::-1]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upMBmwpzIBEN"
      },
      "outputs": [],
      "source": [
        "#Funcao para o POS TAGGING \n",
        "def Postagging(sentence):\n",
        "    phrase = []\n",
        "    for word in sentence:\n",
        "        phrase.append(nltk.pos_tag(word.lower()))\n",
        "    return phrase\n",
        "\n",
        "#cria uma nova coluna no dataframe com o Postagging\n",
        "df['message_postagging'] = df['message_sem_stopwords'].apply(Postagging)\n",
        "df['response_postagging'] = df['response_sem_stopwords'].apply(Postagging)\n",
        "#Exibe o resultado\n",
        "df[df.columns[::-1]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sdzw28qIBHT"
      },
      "outputs": [],
      "source": [
        "#Funcao para stemmingzar (tratar o radical da palavra, até a origem)\n",
        "def Stemming(sentence):\n",
        "    stemmer = PorterStemmer()\n",
        "    phrase = []\n",
        "    for word in sentence:\n",
        "        phrase.append(stemmer.stem(word.lower()))\n",
        "    return phrase\n",
        "\n",
        "#cria uma nova coluna no dataframe com a raiz de tokens\n",
        "df['message_raiz'] = df['message_sem_stopwords'].apply(Stemming)\n",
        "df['response_raiz'] = df['response_sem_stopwords'].apply(Stemming)\n",
        "#Exibe o resultado\n",
        "df[df.columns[::-1]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXBK8nwwJ4CB"
      },
      "outputs": [],
      "source": [
        "#Funcao para Lematizar: processo de agrupar formas flexionadas em uma só palavra com similar.\n",
        "def Lematizar(sentence):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for i in range (len(sentence)):\n",
        "        words = nltk.word_tokenize(sentence[i])\n",
        "        newwords = [lemmatizer.lemmatize(word) for word in words]\n",
        "        sentence[i] = ' '.join(newwords)\n",
        "    return sentence\n",
        "\n",
        "#cria uma nova coluna com o texto com formas flexionadas\n",
        "df['message_lematizado'] = df['message_raiz'].apply(Lematizar)\n",
        "df['response_lematizado'] = df['response_raiz'].apply(Lematizar)\n",
        "#Exibe o resultado\n",
        "df[df.columns[::-1]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXmx8qQSJ4FJ"
      },
      "outputs": [],
      "source": [
        "#Funcao para aplicar o LER nas setencas\n",
        "def Ner(sentence):\n",
        "    doc = nlp(sentence) \n",
        "    phrase = []\n",
        "    for ent in doc.ents:\n",
        "        phrase.append(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "    return phrase\n",
        "\n",
        "#cria uma nova coluna com o texto com as entidades identificadas\n",
        "df['message_NER'] = df['message_lematizado'].apply(Lematizar)\n",
        "df['response_NER'] = df['response_lematizado'].apply(Lematizar)\n",
        "#Exibe o resultado\n",
        "df[df.columns[::-1]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQi9PBnIJ4IJ"
      },
      "outputs": [],
      "source": [
        "#Funcao para aplicar a vetorização TFIDF\n",
        "def vetorizeTFIDF(corpus):\n",
        "    corpus_aux = [list(doc) for doc in corpus]\n",
        "    texts = TextCollection(corpus_aux)      \n",
        "    features = defaultdict(float)\n",
        "    for doc in corpus_aux:\n",
        "        for token in doc:\n",
        "            features[token] = texts.tf_idf(token,doc)\n",
        "    return features \n",
        "\n",
        "#cria uma nova coluna com o texto com as entidades identificadas\n",
        "df['message_vetorizado_TFIDF'] = df['message_lematizado'].apply(vetorizeTFIDF)\n",
        "df['response_vetorizado_TFIDF'] = df['response_lematizado'].apply(vetorizeTFIDF)\n",
        "#Exibe o resultado\n",
        "df[df.columns[::-1]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHqtYD9VVpst"
      },
      "outputs": [],
      "source": [
        "pln = spacy.load('en')\n",
        "pln"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbdKo2XaVpjg"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "stop_words = STOP_WORDS\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXWBRn3BVph1"
      },
      "outputs": [],
      "source": [
        "pontuacoes = string.punctuation\n",
        "pontuacoes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XR-8KEAVppE"
      },
      "outputs": [],
      "source": [
        "len(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNw5oYcrVpvU"
      },
      "outputs": [],
      "source": [
        "#Outra Função para tratar o texto\n",
        "def tratamento_texto(sentence):\n",
        "  \n",
        "  #remove as quebras de linha\n",
        "  sentence = re.sub(r'\\n', '', sentence)\n",
        "  #substitui tabulações por um espaço em branco\n",
        "  sentence = re.sub(r'\\t', ' ', sentence)\n",
        "  #substitui um ou mais espaços em branco por um espaço\n",
        "  sentence= re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
        "  #&amp;\n",
        "  #remove aspas e apóstofres\n",
        "  sentence = re.sub('[\"‘’“”…]', '', sentence)\n",
        "  return sentence\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet\n",
        "\n",
        "def preprocessamento(texto):\n",
        "  texto = texto.lower()\n",
        "  documento = pln(texto)\n",
        "  \n",
        "  lista = []\n",
        "  for token in documento:\n",
        "    #lista.append(token.text)\n",
        "    lista.append(token.lemma_)\n",
        "\n",
        "  lista = [palavra for palavra in lista if palavra not in stop_words and palavra not in pontuacoes]\n",
        "  lista = ' '.join([str(elemento) for elemento in lista if not elemento.isdigit()])\n",
        "\n",
        "  return lista\n",
        "\n",
        "#cria uma nova coluna no dataframe 'sentence' com cada sentence tokenizado\n",
        "df['message_tratado'] = df['message'].apply(tratamento_texto).apply(preprocessamento)\n",
        "df['response_tratado'] = df['response'].apply(tratamento_texto).apply(preprocessamento)\n",
        "#Exibe o resultado\n",
        "df[df.columns[::-1]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbf0u0MvVaDt"
      },
      "outputs": [],
      "source": [
        "#Visualização das informações do Dataset\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQqQOWgWvKc-"
      },
      "outputs": [],
      "source": [
        "\n",
        "sns.countplot(df['entailment_label'], label = 'Inferência das Setenças');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyBa8IatCrui"
      },
      "outputs": [],
      "source": [
        "\n",
        "dias = int(df.shape[0] * 0.2) # 20% do Dataset\n",
        "train_df = df.iloc[:-dias,0:15].copy()\n",
        "val_df = df.iloc[-dias:,0:15].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWz0vMizvPGO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,5))\n",
        "plt.title('Total Inferência das Setenças')\n",
        "plt.plot(train_df['entailment_label']=='POSITIVE', color='b')\n",
        "plt.plot(val_df['entailment_label']=='POSITIVE', color='orange')\n",
        "plt.legend(['Treino','Teste'])\n",
        "# plt.yticks(np.arange(0, 1000, step=50))\n",
        "plt.xlabel('Quantidade')\n",
        "plt.ylabel('Inferencia')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4MPh-GJvR9p"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,5))\n",
        "plt.title('Total Inferência das Setenças')\n",
        "plt.plot(train_df['entailment_label']=='NEGATIVE', color='b')\n",
        "plt.plot(val_df['entailment_label']=='NEGATIVE', color='orange')\n",
        "plt.legend(['Treino','Teste'])\n",
        "# plt.yticks(np.arange(0, 1000, step=50))\n",
        "plt.xlabel('Quantidade')\n",
        "plt.ylabel('Inferencia')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-EvNNXgKxl-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pickle\n",
        "import os\n",
        "import transformers\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9Mz0dUCL6u7"
      },
      "outputs": [],
      "source": [
        "# Requires the latest pip\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Current stable release for CPU and GPU\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYDF4B4FGtEn"
      },
      "source": [
        "#### Model Fitted e Resultados do processo de treinamento (28/06/2021)\n",
        "\n",
        "#### Execução com Held-Out-Data (Dados não vistos) (12/07/2021)\n",
        "\n",
        "#### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRHG7poRYjSb"
      },
      "outputs": [],
      "source": [
        "class MNLIDataBert(Dataset):\n",
        "\n",
        "  def __init__(self, train_df, val_df):\n",
        "    self.label_dict = {'POSITIVE': 0, 'NEGATIVE': 1}\n",
        "\n",
        "    self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "\n",
        "    self.base_path = '/content/'\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
        "    self.tokenizer.save_pretrained('/content/drive/MyDrive/Colab Notebooks/TopicosPLN/Chatbotdataset_bert/')\n",
        "    self.train_data = None\n",
        "    self.val_data = None\n",
        "    self.init_data()\n",
        "\n",
        "  def init_data(self):\n",
        "    self.train_data = self.load_data(self.train_df)\n",
        "    self.val_data = self.load_data(self.val_df)\n",
        "\n",
        "  def load_data(self, df):\n",
        "    MAX_LEN = 512\n",
        "    token_ids = []\n",
        "    mask_ids = []\n",
        "    seg_ids = []\n",
        "    y = []\n",
        "\n",
        "    premise_list = df['message_tratado'].to_list()\n",
        "    hypothesis_list = df['response_tratado'].to_list()\n",
        "    label_list = df['entailment_label'].to_list()\n",
        "\n",
        "    for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
        "      premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
        "      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
        "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
        "      premise_len = len(premise_id)\n",
        "      hypothesis_len = len(hypothesis_id)\n",
        "\n",
        "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
        "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
        "\n",
        "      token_ids.append(torch.tensor(pair_token_ids))\n",
        "      seg_ids.append(segment_ids)\n",
        "      mask_ids.append(attention_mask_ids)\n",
        "      y.append(self.label_dict[label])\n",
        "    \n",
        "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
        "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
        "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
        "    y = torch.tensor(y)\n",
        "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
        "    print(len(dataset))\n",
        "    return dataset\n",
        "\n",
        "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
        "    train_loader = DataLoader(\n",
        "      self.train_data,\n",
        "      shuffle=shuffle,\n",
        "      batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "      self.val_data,\n",
        "      shuffle=shuffle,\n",
        "      batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "  \n",
        "mnli_dataset = MNLIDataBert(train_df, val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_8DLeZ98Ewu"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader = mnli_dataset.get_data_loaders(batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko6MXsBa8K4Z"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bffwM8iKYjVk"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3Od9efw9C7g"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlI4Wdgz9DDp"
      },
      "outputs": [],
      "source": [
        "def multi_acc(y_pred, y_test):\n",
        "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTCYlpw2YjYL"
      },
      "outputs": [],
      "source": [
        "def multi_acc(y_pred, y_test):\n",
        "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
        "  return acc\n",
        "\n",
        "import time\n",
        "\n",
        "EPOCHS = 3\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer):  \n",
        "  total_step = len(train_loader)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_train_acc  = 0\n",
        "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      pair_token_ids = pair_token_ids.to(device)\n",
        "      mask_ids = mask_ids.to(device)\n",
        "      seg_ids = seg_ids.to(device)\n",
        "      labels = y.to(device)\n",
        "\n",
        "      loss, prediction = model(pair_token_ids, \n",
        "                             token_type_ids=seg_ids, \n",
        "                             attention_mask=mask_ids, \n",
        "                             labels=labels).values()\n",
        "\n",
        "      acc = multi_acc(prediction, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      total_train_loss += loss.item()\n",
        "      total_train_acc  += acc.item()\n",
        "\n",
        "    train_acc  = total_train_acc/len(train_loader)\n",
        "    train_loss = total_train_loss/len(train_loader)\n",
        "    model.eval()\n",
        "    total_val_acc  = 0\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
        "        optimizer.zero_grad()\n",
        "        pair_token_ids = pair_token_ids.to(device)\n",
        "        mask_ids = mask_ids.to(device)\n",
        "        seg_ids = seg_ids.to(device)\n",
        "        labels = y.to(device)\n",
        "        \n",
        "        loss, prediction = model(pair_token_ids, \n",
        "                             token_type_ids=seg_ids, \n",
        "                             attention_mask=mask_ids, \n",
        "                             labels=labels).values()\n",
        "        \n",
        "        acc = multi_acc(prediction, labels)\n",
        "\n",
        "        total_val_loss += loss.item()\n",
        "        total_val_acc  += acc.item()\n",
        "\n",
        "    val_acc  = total_val_acc/len(val_loader)\n",
        "    val_loss = total_val_loss/len(val_loader)\n",
        "    end = time.time()\n",
        "    hours, rem = divmod(end-start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### CrossValidation\n",
        "\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# prepare cross validation\n",
        "n=5\n",
        "kf = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
        "\n",
        "results = []\n",
        "\n",
        "for train_index, val_index in kf.split(train_data):\n",
        "  # splitting Dataframe (dataset not included)\n",
        "    train_df = train_data.iloc[train_index]\n",
        "    val_df = train_data.iloc[val_index]\n",
        "    # Defining Model\n",
        "    model = ClassificationModel('bert', 'bert-base-uncased') \n",
        "  # train the model\n",
        "    model.train_model(train_df)\n",
        "  # validate the model \n",
        "    result, model_outputs, wrong_predictions = model.eval_model(val_df, acc=accuracy_score)\n",
        "    print(result['acc'])\n",
        "  # append model score\n",
        "    results.append(result['acc'])\n",
        "\n",
        "\n",
        "print(\"results\",results)\n",
        "print(f\"Mean-Precision: {sum(results) / len(results)}\")"
      ],
      "metadata": {
        "id": "OKWER2TvhpAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh2G4_YDURf6"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.keras.utils.plot_model(reloaded_model)"
      ],
      "metadata": {
        "id": "LNggzW-US13E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwkXdaZI9MoY"
      },
      "outputs": [],
      "source": [
        "#train(model, train_loader, val_loader, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCyldRRUindn"
      },
      "outputs": [],
      "source": [
        "#dataset_name = 'Chatbotdataset'\n",
        "#saved_model_path = path+'{}_bert'.format(dataset_name.replace('/', '_'))\n",
        "\n",
        "#model.save_pretrained(saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o modelo\n",
        "#reloaded_model = tf.saved_model.load(path)\n",
        "#reloaded_model = tf.keras.models.load_model(saved_model_path)\n",
        "#reloaded_model = tf.saved_model.load(saved_model_path)\n",
        "\n",
        "reloaded_model = torch.load('/content/drive/MyDrive/Colab Notebooks/TopicosPLN/Chatbotdataset_bert/pytorch_model.bin',map_location=torch.device('cuda'))"
      ],
      "metadata": {
        "id": "VxDJkTkp2aRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o Tokenizer\n",
        "reloaded_tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/Colab Notebooks/TopicosPLN/Chatbotdataset_bert/')"
      ],
      "metadata": {
        "id": "E_fA7Iq4NDgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip3 uninstall keras\n",
        "#!pip3 install keras --upgrade"
      ],
      "metadata": {
        "id": "onCr5YeBXEEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "#reloaded_metrics = tf.metrics.BinaryAccuracy()"
      ],
      "metadata": {
        "id": "CROZ6y4kSUxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilando o modelo com o optimizador, as perdas e métricas\n",
        "#reloaded_model.compile()\n",
        "#classifier_model.compile(optimizer=optimizer,\n",
        "#                         loss=loss,\n",
        "#                         metrics=metrics)"
      ],
      "metadata": {
        "id": "Ov2uAEIIREOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9YkZ6QCTYEp"
      },
      "outputs": [],
      "source": [
        "def plot_scores(history):\n",
        "    acc = [x['val_acc'] for x in history]\n",
        "    plt.plot(acc, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('acc')\n",
        "    plt.title('acc vs. No. of epochs');\n",
        "\n",
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaIyhxn7Thtc"
      },
      "outputs": [],
      "source": [
        "#plot_losses(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y12K-NEnz0HZ"
      },
      "outputs": [],
      "source": [
        "def get_prediction(str):\n",
        " str = re.sub(r'[^a-zA-Z ]+', '', str)\n",
        " test_text = [str]\n",
        " model.eval()\n",
        " \n",
        " tokens_test_data = mnli_dataset.tokenizer(\n",
        " test_text,\n",
        " pad_to_max_length=True,\n",
        " truncation=True,\n",
        " return_token_type_ids=False\n",
        " )\n",
        " test_seq = torch.tensor(tokens_test_data['input_ids'])\n",
        " test_mask = torch.tensor(tokens_test_data['attention_mask'])\n",
        " \n",
        " preds = None\n",
        " with torch.no_grad():\n",
        "   preds = model(test_seq.to(device), test_mask.to(device))\n",
        "\n",
        " preds = np.argmax(preds)\n",
        " print(\"Intent Identified: \", le.inverse_transform(preds)[0])\n",
        " return le.inverse_transform(preds)[0]\n",
        "\n",
        "def get_response(message): \n",
        "  intent = get_prediction(message)\n",
        "  for i in data['intents']: \n",
        "    if i[\"tag\"] == intent:\n",
        "      result = random.choice(i[\"responses\"])\n",
        "      break\n",
        "  print(f\"Response : {result}\")\n",
        "  return \"Intent: \"+ intent + '\\n' + \"Response: \" + result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7AsaASCOBvi"
      },
      "outputs": [],
      "source": [
        "#get_response(\"Fazer outra pergunta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNS_zIXRftBc"
      },
      "outputs": [],
      "source": [
        "#predictions = model.predict(x_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "TAPLN_ProjetoFinal.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}